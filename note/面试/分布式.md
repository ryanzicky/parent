### 1. 雪花算法

snowflake-64bit

```
0-00000000 00000000 00000000 00000000 00000000 - 00000000 00 - 00000000 0000
第1位：1bit-不用
第2位到第42位：41bit-时间戳
第43位到第52位：10bit-工作机器id
第53位到第64位：12bit-序列号
```

第一位符号位固定为0，41位时间戳，10为workId，12位序列号，位数可以有不同优点：

-   每个毫秒值包含的id值很多，不够可以变动位数来增加，性能佳（依赖workId的实现）
-   时间戳值在高位，中间是固定的机器码，自增的序列在低位，整个ID是趋势递增的。
-   能够根据业务场景数据库节点位置灵活调整bit划分，灵活度高。

缺点：

-   强依赖于机器时钟，如果时钟回拨，会导致重复的ID生成，所以一般基于此的算法发现时钟回拨，都会抛出异常，阻止ID生成，这可能导致服务不可用。

### 2. 为什么Zookeeper可以用来作为注册中心

可以利用Zookeeper的临时节点和watch机制来实现注册中心的自动注册和发现，另外Zookeeper中的数据都是存在内存中的，并且Zookeeper底层采用了nio，多线程模型，所以Zookeeper的性能也是比较高的，所以可以用来作为注册中心。但是如果考虑到注册中心应该是注册可用性的话，那么Zookeeper则不太合适，因为Zookeeper是CP的，它注重的是一致性，所以集群数据不一致时，集群将不可用，所以用Redis，Eureka，Nacos来作为注册中心将更合适。

### 3. 数据库实现分布式锁的问题及解决方案

利用唯一约束建存储key，insert成功则代表获取锁成功，失败则获取失败，操作完成需要删除锁。

问题：

-   非阻塞，锁获取失败后没有排队机制，需要自己编码实现阻塞，可以使用自旋，直到获取锁
-   不可重入，如果加锁的方法需要递归，则第二次插入会失败，可以使用记录线程标识解决重入锁问题
-   死锁，删除锁失败，则其他线程没办法获取锁，可以设置超时时间，使用定时任务检查
-   数据库单点故障，数据高可用

### 4. 什么是分布式事务？有哪些实现方案

解决方案：

1.  本地消息表：
    1.  消息生产者将消息加入到本地消息中，一起提交到数据库存入本地消息表，然后调用消费者接口，消费者接口调用成功，修改消息状态为成功，反之失败。
2.  消息队列：RocketMQ支持事务消息，工作原理：
    1.  生产者发送一条half消息的Broker，half消息对消费者不可见
    2.  生产者完成后续逻辑，根据结果，向Broker发送conmmit或rollback
    3.  生产者还可以提供Broker回调接口，当Broker发现一段时间half消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功
    4.  一旦half消息commit了，消费者系统就来消费，如果消费成功，消息销毁，分布式事务结束
    5.  如果消费失败，则根据重试策略进行重试，最后还失败则进入死信队列，等待进一步处理。
3.  Seata：阿里开源的分布式事务框架，支持AT,TCC等多种模式，底层都是基于两阶段提交理论来实现的。

### 5. 为什么Dubbo不用JDK的SPI，而是要自己实现？

Java Spi缺点：

	1. 需要遍历所有实现并实例化，假设一个实现类初始化过程比较消耗资源且耗时，但是代码里面又用不上，这就产生了资源的浪费，也无法准确引用。
	2. 没有使用缓存每次load都需要重新加载。

Dubbo SPI：

1.  给每个实现类配了个名字，通过名字去文件里面找到对应的实现类全限定名然后加载实例化，按需加载。
2.  增加了缓存存储实例，提高读取性能。
3.  提供了对IOC和AOP等高级功能的支持，以实现更多类型的扩展。

### 6. 什么是ZAB协议

ZAB协议是Zookeeper用来实现一致性的原子广播协议，该协议描述了Zookeeper是如何实现一致性的，分为三个阶段：

1.  领导者选举阶段：从Zookeeper集群中选出一个节点作为Leader，所有的写请求都会有Leader节点来处理
2.  数据同步阶段：集群中所有节点中的数据要和Leader节点保持一致，如果不一致则要进行同步
3.  请求广播阶段：当Leader节点接收到写请求时，会利用两阶段提交来广播该写请求，使得写请求像事务一样在其他节点上执行，达到节点上的数据实时一致

但值得主意的是，Zookeeper只是尽量的在达到强一致性，实际上仍然只是最终一致性的。

### 7. 如何设计一个分布式锁？如何对锁性能进行优化？

分布式锁的本质：就是在所有进程都能访问到的地方，设置一个锁资源，让这些进程都来竞争锁资源，数据库，Zookeeper，Redis，通常对于分布式锁，会要求响应快，性能高，与业务无关。

Redis实现分布式锁：

SETNX key value：当key不存在时，就将key设置为value，并返回1。如果key存在，就返回0。

EXPIRE key locktime：设置key的有效市场。DEL key：删除。GETSET key value：先GET，再SET，

先返回key对应的值，如果没有就返回空。然后在讲key设置成value。

1.  最简单的分布式锁：

    SETNX 加锁，DEL解锁。问题：如果获取到锁的进程执行失败，它就永远不会主动解锁，那这个锁就被锁死了。

2.  给锁设置过期时长:

    问题：SETNX和EXPIRE并不是原子性的，所以获取到锁的进程有可能还没有执行EXPIRE指令，就挂了，这时锁还是会被锁死。

3.  将锁的内容设置为过期时间（客户端时间+过期时长），SETNX获取锁失败时，拿这个时间跟当前时间比对，如果是过期的锁，就先删除锁，再重新上锁。问题：在高并发场景下，会产生多个进程同时拿到锁的情况。

4.  SETNX失败后，获取锁上的时间戳，然后用getset，将自己的过期时间更新上去，并获取旧值。如果这个旧值，跟之前获得的时间戳是不一致的，就表示这个锁已经被其他进程占用了，自己就要放弃竞争锁。

    ```java
    // 加锁
    public boolean tryLock(RedisConnection conn) {
    	long nowTime = System.currentTimeMillis();
        long expireTime = nowTime + 1000;
        if (conn.SETNX("mykey", expireTime) == 1) {
            conn.EXPIRE("mykey", 1000);
            return true;
        } else {
            long oldVal = conn.get("mykey");
            if (oldVal != null && oldVal < nowTime) {
                long currentVal = conn.GETSET("mykey", expireTime);
                if (oldVal == currentVal) {
                    conn.EXPIRE("mykey", 1000);
                }
                return true;
            }
            return false;
        }
    }
    
    // 解锁
    DEL
    ```

    

5.  上面就行程了一个比较高效的分布式锁。分析一下，上面各种优化的根本问题在于SETNX和EXPIRE两个指令无法保证原子性。Redis2.6提供了直接执行lua脚本的方式，通过lua脚本来保证原子性。redisson。

### 6. 什么是CAP理论

CAP理论是分布式领域中非常重要的一个指导理论，C(Consisitency)表示强一致性，A(Availability)表示可用性，P(Partition Tolerance)表示分区容错性，CAP理论指出在目前的硬件条件下，一个分布式系统是必须要保证分区容错性的，而在这个前提下，分布式系统要么保证CP，要么保证AP，无法同时保证CAP。

分区容错性表示，一个系统虽然是分布式的，但是对外看上去应该是一个整体，不能由于分布式系统内部的某个节点挂掉，或网络出现了故障，而导致系统对外出现异常，所以，对于分布式系统而言是一定要保证分区容错性的。

强一致性表示，一个分布式系统中各个节点之间能及时同步数据，在数据同步过程中，是不能对外提供服务的，不然会造成数据不一致，所以强一致性和可用性是不能同时满足的。

可用性表示，一个分布式系统对外要保证可用。

### 7. 什么是RPC

RPC，表示远程过程调用，对于Java这种面向对象语言，也可以理解为远程方法调用，RPC调用和HTTP调用是有区别的，RPC表示的是一种调用远程方法的方式，可以使用HTTP协议，或直接基于TCP协议来实现RPC，在Java中，我们可以通过直接使用某个服务接口的代理对象来执行方法，而底层则通过构造HTTP请求来调用远端的方法，所以，有一种说法是RPC协议是HTTP协议之上的一种协议，也是可以理解的。

### 8. 如何实现接口幂等性

1.  唯一id：生成唯一的id，执行前判断id是否存在，如果不存在则执行后续操作，并且保存到数据库或者redis等。
2.  服务端提供发送token的接口，服务器判断token是否在tedis中，执行完后，需要把token删除。
3.  建去重表：将业务中有唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了。
4.  版本控制：增加版本号，当版本号符合时，才能更新数据
5.  状态控制：例如订单有状态已支付，未支付，支付中，支付失败，当处于未支付的时候才允许修改为支付中等。

### 9. 什么是BASE理论

1.  BA：Basically Available，表示基本可用，表示可以允许一定程度的不可用，比如由于系统故障，请求时间变长，或者由于系统故障导致部分非核心功能不可用，都是允许的。
2.  S：Soft State：表示分布式系统可以处于一种中间状态，比如数据正在同步。
3.  E：Eventually consisitent，表示最终一致性，不要求分布式系统数据实时达到一致，允许在经过一段时间后再达到一致，在达到一致过程中，系统也是可用的。

### 10. Paxos算法

Paxos算法解决的是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个几点执行相同的操作序列，那么它们最后能够得到一个一致的状态。为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。在Paxos算法中，有三种角色：Proposer（提议者），Acceptor（接受者），Learners（记录员）。

-   Proposer提议者：只要Proposer发的提案Propose被半数以上的Acceptor接受，Proposer就认为该提案例的value被选定了。
-   Acceptor接受者：只要Acceptor接受了某个提案，Acceptor就认为该提案例的value被选定了
-   Learner记录员：Acceptor告诉Learner哪个value被选定了，Learner就认为哪个value被选定了

Paxos算法分为两个阶段，具体如下：

阶段一（prepare）：

1.  Proposer收到client请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求
2.  Acceptor收到一个编号为N的Prepare请求，如果该轮paxos
    1.  本节点已经有已提交的value记录，对比记录的编号和N，大于N则拒绝回应，否则返回该记录value及编号。
    2.  没有已提交记录，判断本地是否有编号N1，N1 > N，则拒绝响应，否则将N1改为N（如果没有N1，则记录N），并响应prepare。

阶段二（accept）：

1.  如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N，V]提案的Accept请求给半数以上的Acceptor。V就是收到的响应中编号最大的value，如果响应中不包含任何value，那么V就由Proposer自己决定。
2.  如果Acceptor收到一个针对编号为N的提案的Accept请求，Acceptor对比本地的记录编号，如果小于等于N，则接受该值，并提交记录value。否则拒绝请求。

Proposer如果收到的大多数Acceptor响应，则选定该value值，并同步给learner，使未响应的Acceptor达成一致。

活锁：accept时被拒绝，加大N，重新accept，此时另外一个proposer也进行相同操作，导致accept一直失败，无法完成算法。

multi-paxos：区别于paxos只是确定一个值，multi-paxos可以确定多个值，收到accept请求后，则一定时间内不再accept其他节点的请求，以此保证后续的编号不需要在经过prepare确认，直接进行accept操作。此时该节点成为了leader，知道accept被拒绝，重新发起prepare请求竞争leader资格。

### 11. 什么RPC

远程过程调用

调用方像调用内部接口一样调用远程的方法，而不用封装参数名和参数值的等操作。

包含：

	1. 动态代理，封装调用细节
	2. 序列化与反序列化，数据传输与接收
	3. 通信，可以选择七层http，四层的tcp/udp
	4. 异常处理等

首先，调用方调用的是接口，必须得外接口构造一个假的实现。显然，要使用动态代理。这样，调用方的调用就被动态代理接收到了。

第二，动态代理接收到调用后，应该想办法调用远程的实际实现。这包括下面几步：

1.  识别具体要调用的远程方法的IP，接口
2.  将调用方法的入参进行序列化
3.  通过通信将请求发送到远程的方法中

这样，远程的服务就接收到了调用方的请求。它应该：

1.  反序列化各个调用参数
2.  定位到实际要调用的方法，然后输入参数，执行方法
3.  按照调用的路径返回调用的结果

### 12. Zookeeper对事务性的支持

Zookeeper对于事务性的支持主要依赖于这四个函数，zoo_create_op_init，zoo_delete_op_init，zoo_set_op_init以及zoo_check_op_init。

每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations。当准备好一个事务中的所有操作后，可以使用zoo_multi来提交所有的事务，由zookeeper服务来保证这一系列操作的原子性。也就是说只要其中一个操作失败了，相当于此次提交的任何一个操作都没有对服务端的数据造成影响，zoo_multi的返回值是第一个失败操作的状态信号。

### 13. Dubbo工作流程

![Dubbo工作流程](%E5%88%86%E5%B8%83%E5%BC%8F.assets/Dubbo%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

1.  Start：启动Spring容器时，自动启动Dubbo的Provider
2.  Register：Dubbo的Provider在启动后会去注册中心注册内容，注册的内容包括：IP，端口，接口列表（接口类，方法），版本，Provider的协议
3.  Subscribe：订阅，当Consumer启动时，自动去Registry获取到所有已注册的服务的信息
4.  Notify：通知，当Provider的信息发生变化时，自动由Registry向Consumer推送通知
5.  Invoke：Consumer调用Provider中方法
    1.  同步请求：消耗一定性能，但是必须是同步请求，因为需要接收调用方法后的结果
6.  Count：次数，每隔2分钟，Provider和Consumer自动向Monitor发送访问次数，Monitor进行统计。

### 14. RPC，RMI的理解

RPC：在本地调用远程的函数，远程过程调用，可以跨语言实现，httpClient

RMI：远程方法调用，java中用于实现RPC的一种机制，RPC的java版本，是J2EE的网络调用机制，跨JVM调用对象的方法，面向对象的思维方式

直接或间接实现接口 java.rmi.Remote 成为存在于服务器端的远程对象，供客户端访问并提供一定的服务

远程对象必须实现 java.rmi.server.UniCastRemoteObject类，这样才能保证客户端访问获得远程对象时，该远程对象将会把自身的一个拷贝以Socket的形式传输给客户端，此时客户端所获得的这个拷贝成为“存根”，而服务器端本身已存在的远程对象则称之为“骨架”。其实此时的存根是客户端的一个代理，用于与服务器端的通信，而骨架也可认为是服务器端的一个代理，用于接收客户端的请求之后调用远程方法来响应客户端的请求。

### 15. Raft算法

概念：

-   分布式一致性算法：raft会先选举出leader，leader完全负责replicated log的管理。leader负责接受所有客户端更新请求，然后复制到follower节点，并在“安全”的时候执行这些请求。如果leader故障，followers会重新选举出新的leader。
-   三种状态：一个节点任一时刻处于三者之一
    -   leader：处理所有的客户端请求（如果客户端将请求发给了Follower，Follower将请求重新定向给Leader）
    -   follower：不会发送任何请求，只会简单地响应来自Leader或Candidate的请求
    -   candidate：用于选举产生新的Leader（候选人）
-   term：任期，leader产生到重新选举为一任期，每个节点都维持着当前的任期号
    -   

何时触发选举：

-   集群初始化时，都是follower，随机超时，编程candidate，发起选举
-   如果follower在*election timeout*内没有收到来自leader的心跳，则主动触发选举

选举过程：发出选举的节点角度

1.  增加节点本地的term，切换的candidate状态

2.  投自己一票

    其他节点投票逻辑：每个节点同一任期最多只能投一票，候选人知道的信息不能比自己少（通过副本日志和安全机制保障），先来先得



### 16. Zookeeper watch机制

客户端，可以通过在znode上设置watch，实现实时监听znode的变化。

Watch事件是一个一次性的触发器，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端。

-   父节点的创建，修改，删除都会触发Watch事件
-   子节点的创建，删除会触发Watch事件

一次性：一旦被触发就会移除，再次使用需要重新注册，因为每次变动都需要通知所有客户端，一次性可以减轻压力，3.6.0默认持久递归，可以触发多次。

轻量：只通知发生了事件，不会告知事件内容，减轻服务器和贷款压力。

Watcher机制包括三个角色：客户端线程，客户端的WatchManager以及Zookeeper服务器。

1.  客户端向Zookeeper服务器注册一个Watcher监听。
2.  把这个监听信息存储到客户端的WatchManager中。
3.  当Zookeeper中的节点发生变化时，会通知客户端，客户端会调用相应Watcher对象中的回调方法。watch回调是串行同步的。

### 17. Zookeeper中的观察者机制

```
peerType=observer
server.1:localhost:2181:3181:observer
```

观察者的设计是希望能动态扩展Zookeeper集群又不会降低写性能。

如果扩展节点是follower，则写入操作提交时需要同步的节点数会变多，导致写入性能下降，而follower又是参与投票的，也会导致投票成本增加

observer是一种新的节点类型，解决扩展问题的同时，不参与投票，只获取投票结果，同时也可以处理读写请求，写请求转发给leader。负责接收leader同步过来的提交数据，observer的节点故障也不会影响集群的可用性。

跨数据中心部署，把节点分散到多个数据中心可能因为网络的延迟会加大拖慢系统。使用observer的话，更新操作都在一个单独的数据中心来处理，并发送到其他数据中心，让其他数据中心的节点消费数据。

无法完全消除数据中心之间的网络延迟，因为observer需要把更新请求转发到另一个数据中心的leader，并处理同步消息，网络速度极慢的话也有影响，它的优势是为本地读请求提供快速响应。

### 18. ZAB协议

ZAB协议是为分布式协调服务Zookeeper专门设计的一种支持崩溃恢复的原子广播协议，实现分布式数据一致性。

所有客户端的请求都是写入到Leader的进程中，然后，由Leader同步到其他节点，成为Follower。在集群数据同步的过程中，如果出现Follower节点崩溃或者Leader进程崩溃时，都会通过ZAB协议来保证数据一致性。

ZAB协议包括两种基本的模式：崩溃恢复和消息广播

消息广播：

-   集群中所有的事务请求都由Leader节点来处理，其他服务器为Follower，Leader将客户端的事务请求转换为事务Proposal，并且将Proposal分发给集群中其他所有的Follower。
-   完成广播之后，Leader等待Follower反馈，当有过半数的Follower反馈信息后，Leader将再次向集群内Follower广播Commit信息，Commit信息就是确认将之前的Proposal提交。
-   Leader节点的写入是一个两步操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数指的是反馈的节点数>=N/2+1，N是全部的Follower节点数量。

崩溃恢复：

-   初始化集群，刚刚启动的时候
-   Leader崩溃，因为故障宕机
-   Leader失去了半数的机器支持，与集群中超过一般的节点断连

此时开启新一轮Leader选举，选举产生的Leader会与过半的Follower进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式，然后进入消息广播模式。

整个Zookeeper集群的一致性保证就是在上面两个状态之间切换，当Leader服务正常时，就是正常的消息广播模式；当Leader不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段。

Zxid是Zab协议的一个事务编号，Zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端每一个事务请求，计数器加1；而高32位则代表Leader周期年代的编号。

Leader周期（epoch），可以理解为当前集群所处的年代或者周期，每当有一个新的Leader选举出现时，就会从这个Leader服务器上取出其本地日志中最大事务的Zxid，并从中读取epoch值，然后加1，以此作为新的周期ID。高32位代表了每代Leader的唯一性，低32位代表了每代Leader中事务的唯一性。

ZAB节点的三种状态：

following：服从leader的命令

leading：负责协调事务

election/looking：选举状态

### 19. Zk的数据模型

ZK的数据模型是一种树形结构，具有一个固定的根节点（/），可以在根节点下创建子节点，并在子节点下继续创建下一级节点。每一层级用/隔开，且只能用绝对路径（get /work/task1）的方式查询ZK节点，而不能用相对路径。

-   持久节点：

    将节点创建为持久节点，该数据节点会一直存储在ZK服务器上，即使创建该节点的客户端与服务端的会话关闭了，该节点依然不会被删除，除非显示调用delete函数进行删除操作。

-   临时节点：

    如果将节点创建为临时节点，那么该节点数据不会一直存储在ZK服务器上。当创建该临时节点的客户端会话因超时或发生异常而关闭时，该节点也相应在ZK服务器上被删除。也可以主动调用delete删除。

-   有序节点：

    有序节点并不算一种单独种类的节点，而是在持久节点和临时节点的基础上，增加一个节点有序的性质。创建有序节点的时候，ZK服务器会自动使用一个单调递增的数字作为后缀，追加到创建节点后边。例如一个客户端创建了一个路径为work/task-的有序节点，那么Zookeeper将会生成一个序号并追加到该节点的路径后，最后该节点的路径为work/task-1。

节点内容：一个二进制数组（byte data[]），用来存储节点的数据，ACL访问控制，子节点数据（因为临时节点不允许有子节点，所以其子节点字段为null），记录自身状态信息的stat。

stat + 节点路径可以查看状态信息。

czxid：创建节点的事务id

mzxid：最后一次被更新的事务id

pzxid：子节点最后一次被修改的事务id

ctime：创建时间

mtime：最后更新时间

version：版本号，表示的是对节点数据内容，子节点信息或ACL信息的修改次数，可以避免并发更新问题，使用之前获取的版本进行CAS操作更新

cversion：子节点版本号

aversion：acl的版本号

ephemeralOwner：创建节点的sessionId，如果是持久节点，值为0

dataLength：数据内容长度

### 20. ZK的命令服务，配置管理，集群管理

命令服务：

通过指定的名字来获取资源或者服务地址。Zookeeper可以创建一个全局唯一的路径，这个路径就可以作为一个名字。被命名的实体可以是集群中的机器，服务的地址，或者是远程的对象等。一些分布式服务框架（RPC，RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体，服务地址和提供者信息等。

配置管理：

实际项目开发中，经常使用.properties或者xml需要配置很多信息，如数据库连接信息，fps地址端口等等。程序分布式部署时，如果把程序的这些配置信息保存在zl的znode节点下，当你要修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，从而更改配置。

集群管理：

集群管理包括集群监控和集群控制，就是监控集群机器状态，提出机器和加入机器。Zookeeper可以方便集群机器的管理，它可以实时监控znode节点的变化，一旦发现有机器挂了，该机器就会与zk断开连接，对应的临时目录节点会被删除，其他所有机器都收到通知。新机器加入也是类似。

### 21. ZK的典型应用场景

通过对Zookeeper中丰富的数据节点进行交叉使用，配合Watcher事件通知机制，可以非常方便的构建一系列分布式应用中会涉及的核心功能，如：

1.  数据发布/订阅：配置中心
2.  负载均衡：提供服务者列表
3.  命名服务：提供服务名到的服务地址的映射
4.  分布式协调/通知：watch机制和临时节点，获取各节点的任务进度，通过修改节点发出通知
5.  集群管理：是否有机器退出和加入，选举master
6.  分布式锁
7.  分布式队列

第一类，在约定目录下创建临时目录节点，监听节点数目是否是要求的数目

第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISITENE_SEQUENTIAL节点，创建成功时Watcher通知等待队列，队列删除序列号最小的节点用以消费。此场景下Zookeeper的Znode用于消息存储，Znode存储的数据就是消息队列中的消息内容，SEQUENTIAL序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。

### 22. TCC事务模型

TCC（补偿事务）：Try，Confirm，Cancel

针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作

Try操作做业务检查及资源预留，Confirm做业务确认操作，Cancel实现一个与Try相反的操作即回滚操作。TM首先发起所有的分支事务的try操作，任何一个分支事务的try操作执行失败，TM将会发起所有分支事务的Cancel操作，若try操作全部成功，TM将会发起所有分支事务的Commit操作，其中Commit/Cancel操作若执行失败，TM会进行重试。

TCC模式对业务的侵入性较强，改造的难度较大，每个操作都需要有`try`，`confirm`，`cancel`三个接口实现。

TCC中会添加事务日志，如果Confirm或者Cancel阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要人工介入进行恢复和处理等。

### 23. Dubbo服务暴露过程

Dubbo采用URL的方式来作为约定的参数类型。

```java
protocol: //username:password@host:port/path?key=value&key=value
protocol: 指的是dubbo中的各种协议，如：dubbo thrift http
username/password: 用户名/密码
host/port:主机/端口
path: 接口的名称
parameters: 参数键值对
```

ServiceBean实现了ApplicationListener，监听ContextRefreshedEvent事件，在Spring IOC容器刷新完成后调用onApplicationEvent方法，服务暴露的启动点。根据配置得到URL，再利用Dubbo SPI机制根据URL的参数选择对应的实现类，实现扩展。

通过javassist动态封装服务实现类，统一暴露出Invoker使得调用方便，屏蔽底层实现细节，然后封装成exporter存储起来，等待消费者的调用，并且会将URL注册到注册中心，使得消费者可以获取服务提供者的信息。

一个服务如果有多个协议那么就都需要暴露，比如同时支持dubbo协议和hessian协议，那么需要将这个服务用两种协议分别向多个注册中心（如果有多个的话）暴露注册。

1.  检测配置，如果有些配置空的话会默认创建，并且组装成URL。
2.  根据URL进行服务暴露，创建代理类Invoker，根据URL得知具体的协议，根据Dubbo SPI选取实现类实现exporter。
3.  如果只是本地暴露，将exporter存入ServiceConfig的缓存。
4.  远程暴露，先通过registry协议找到RegistryProtocol进行export，将URL中export=dubbo://...先转换成exporter，然后获取注册中心的相关配置，如果需要注册则向注册中心注册，并且在ProviderConsumerRegTable这个表格中记录服务提供者，其实就是往一个ConcurrentHashMap中将塞入invoker，key就是服务接口全限定名，value是一个set，set里面会存包装过的invoker，根据URL上Dubbo协议暴露出exporter，打开Server调用NettyServer来监听服务。

### 24. Dubbo服务引入过程

饿汉式是通过调用ReferenceBean的afterPropertiesSet方法时引入服务。

饿汉式是只有当这个服务被注入到其他类中时启动引入流程，也就是说用到了才会开始服务引入。默认使用懒汉式，如果需要使用饿汉式，可通过配置dubbo:reference的init属性开启。

ReferenceBean实现了FactoryBean接口，当对任意服务Interface进行自动注入或者getBean获取时，就会触发getObject()函数的服务引用过程。

-   本地引入走injvm协议，到服务暴露的缓存中去exporter。
-   直连远程引入服务，测试的情况下用，不需要启动注册中心，由Consumer直接配置写死Provider的地址，然后直连即可。
-   注册中心引入远程服务，Consumer通过注册中心得知Provider的相关信息，然后进行服务的引入。

获取注册中心实例，向注册中心注册自身，并订阅providers，configurations，routers节点，触发DubboInvoker的生成，cluster将多个服务调用者进行封装，返回一个invoker。

通过配置构件一个map，然后利用map来构建URL，再通过URL上的协议利用自适应扩展机制调用对应的protocol.refer得到相应的invoker，然后再构建代理，封装invoker返回服务引用，之后Consumer调用这个代理类。

### 25. Dubbo SPI机制

SPI（Service Provider Interface）：服务发现机制

通过接口全限定名找到指定目录下对应的文件，获取具体的实现类然后加载即可，做到了灵活的替换具体的实现类。

![Dubbo流程图](%E5%88%86%E5%B8%83%E5%BC%8F.assets/Dubbo%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

### 26. 服务调用过程

调用某个接口的方法会调用之前生成的代理类，然后会从cluster中经过路由的过滤，负载均衡机制选择一个invoker发起远程调用，此时会记录此请求和请求的ID等待服务端的相应。

服务端接受请求之后会通过参数找到之前暴露存储的map，得到相应的exporter，然后最终调用真正的实现类，再组装好结果返回，这个响应会带上之前请求的ID。

消费者受到这个响应之后会通过ID去找之前记录的请求，然后找到请求之后将响应塞到对应的Future中，唤醒等待的线程，最后消费者得到响应。

### 27. CAP理论

### 28. 集群，分布式，SOA，微服务的概念及区别

-   集群：不同服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或者设备（热备，主从等），指同一种组件的多个实例，行程的逻辑上的整体。单个节点可以提供完整服务。集群是物理形态。
-   分布式：服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，需要多节点协调提供服务（也可以是相同组件部署在不同节点，但节点间通过交换信息写作提供服务），分布式强调的是工作方式。
-   SOA：面向服务的架构，一种设计方式，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在与操作系统进程中，各个服务之间通过网络调用。
    -   中心化实现：ESB（企业服务总线），各服务通过ESB进行交互，解决异构系统之间的连通性，通过协议转换，消息解析，消息路由把服务提供者的数据传送到服务消费者。很重，有一定的逻辑，可以解决一些公用逻辑的问题。
    -   去中心化实现：微服务
-   微服务：在SOA上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个独立开发，设计，运行的小应用。这些小应用之间通过服务完成交互和继承。

服务单一职责

轻量级通信：去掉ESB总线，采用restapi通信。

### 29. Dubbo分层设计

![Dubbo分层设计](%E5%88%86%E5%B8%83%E5%BC%8F.assets/Dubbo%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1.png)

-   Service：业务层，就是我们发开的业务逻辑层
-   Config：配置层，主要围绕ServiceConfig和ReferenceConfig，初始化配置信息
-   Proxy：代理层，服务提供者还是消费者都会生成一个代理类，使得服务接口透明化，代理层做远程调用和返回结果。
-   Register：注册层，封装了服务注册和发现。
-   Cluster：路由和集群容错层，负责选取具体调用的节点，处理特殊的调用要求和负责远程调用失败的容错措施。
-   Monitor：监控层，负责监控统计调用时间和次数。
-   Protocol：远程调用层，主要是封装RPC调用，主要负责管理Invoker。
-   Exchange，信息交换层，用来封装请求响应模型，同步转异步。
-   Transport：网络传输层，抽象了网络传输的统一接口，Netty，Mina等。
-   Serialize：序列化层，将数据序列化成二进制流，以及反序列化。

### 30. Base理论

CAP理论的一种妥协，由于CAP只能二取其一，Base理论降低了发生分区容错时对可用性和一致性的要求。

1.  基本可用：允许可用性降低（可能相应延长，可能服务降级）
2.  软状态：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性。
3.  最终一致性：节点数据同步可以存在时延，但在一个的期限后必须达成数据的一致，状态那边为最终状态。

### 31. 负载均衡策略

1.  轮询法：

    将请求按顺序轮流地分配到后端服务器上，均衡对待每一台服务器，不关心服务器实际的连接数和当前的系统负载。

2.  加权轮询法：

    给配置高，负载低的机器配置更高的权重；配置低，负载高的机器，配置较低的权重。

3.  随机法：

    通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。

4.  加权随机法：

    按照权重随机请求后端服务器。

5.  源地址哈希法：

    根据获取客户端的IP地址，通过哈希函数计算得到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客户端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台服务器进行访问。

6.  最小连接数法：

    最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用率，将负责合理地分流到每一台服务器。

### 32. 负载均衡算法，类型

类型：

DNS方式实现负载均衡

硬件负载均衡：F5和A10

软件负载均衡：

`Nginx`，`HAproxy`，`LVS`。其中的区别：

-   `Nginx`：七层负载均衡，支持HTTP，E-mail协议，同时也支持4层负载均衡；
-   `HAproxy`：支持七层规则的，性能也很不错。OpenStack默认使用的负载均衡软件就是HAproxy；
-   `LVS`：运行在内核态，性能是软件负载均衡中最高的，严格来说工作在三层，所以更通用一些，使用各种应用服务。

### 33. 分布式锁解决方案

需要这个锁独立于每个服务之外，而不是在服务里面、

数据库：利用主键冲突控制一次只有一个线程能获取锁，非阻塞，不可重入，单点，失效时间。

Zookeeper分布式锁：

```java
zk通过临时节点，解决了死锁的问题，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除，其他客户端自动获取锁。临时顺序节点解决惊群效应。
```

Redis分布式锁：setNX，单线程处理网络请求，不需要考虑并发安全性。

所有服务节点设置相同的key，返回为0，则锁获取失败。

```java
setNX
问题：
	1. 早期版本没有超时参数，需要单独设置，存在死锁问题（中途宕机）
	2. 后期版本提供加锁与设置时间原子操作，但是存在任务超时，锁自动释放，导致并发问题，加锁与释放锁不是同一线程问题
```

删除锁：判断线程唯一标识，再删除

可重入锁及锁续期没有实现，通过redisson解决（类似AQS的实现，看门狗监听机制）

redlock：意思的机制都只操作单节点，即使Redis通过sentinel保证高可用，如果这个master节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况（redis同步设置可能数据丢失）。redlock从多个节点申请锁，当一半以上节点获取成功，锁才算获取成功，redisson有相应的实现。

### 34. 分布式系统的设计目标

可扩展性：通过对服务，存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。

高可用：单点不影响整体，单点故障系统中某个组件一旦失效，会让整个系统无法工作。

无状态：无状态的服务才能满足部分机器宕机不影响全部，可以随时进行扩展的需求。

可管理：便于运维，出问题能不能及时发现定位。

高可靠：同样的请求返回同样的数据；更新能够持久化；数据不会丢失。

### 35. 分布式事务如何处理？怎么保证事务一致性？

分布式事务：就是要将不同节点上的事务操作，提供操作原子性保证，同时成功或者同时失败。

分布式事务第一个要点就是要在原本没有直接关联的事务之间建立联系。

1.  HTTP连接：最大努力通知。 ----- 事后补偿
2.  MQ：事务消息机制。
3.  Redis：也可以定制出分布式事务机制。
4.  Seata：是通过TC来在多个事务之间建立联系。

量阶段：AT XA 在于要锁资源

三阶段：TCC 在两阶段的基础上增加一个准备阶段，在准备阶段是不锁资源的

SAGA模式：类似于熔断，业务自己实现正向操作和补偿操作的逻辑

### 36. 分布式事务解决方案

XA规范：分布式事务规范，定义了分布式事务模型

四个角色：事务管理器（协调者TM），资源管理器（参与者RM），应用程序AP，通信资源管理器CRM

全局事务：一个横跨多个数据库的事务，要么全部提交，要么全部回滚

JTA事务是Java对XA规范的实现，对应JDBC的单库事务

两阶段协议：

![两阶段协议](%E5%88%86%E5%B8%83%E5%BC%8F.assets/%E4%B8%A4%E9%98%B6%E6%AE%B5%E5%8D%8F%E8%AE%AE.png)

第一阶段（prepare）：每个参与者执行本地事务但不提交，进入ready状态，并通知协调者已经准备就绪。

第二阶段（commit）：当协调者确认每个参与者都ready后，通知参与者进行commit操作；如果有参与者fial，则发送rollback命令，各参与者做回滚。

问题：

-   单点故障：一旦事务管理器出现故障，整个系统不可用（参与者都会阻塞住）
-   数据不一致：在阶段二，如果事务管理器只发送了部分commit消息，此时网络发生异常，那么只有部分参与者接收到commit消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
-   响应时间较长：参与者和协调者资源都被锁住，提交或者回滚之后才能释放。
-   不确定性：当协调事务管理器发送commit之后，并且此时只有一个参与者收到了commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。

三阶段协议：主要是针对两阶段的优化，解决了2PC单点故障的问题，但是性能问题和不一致问题任然没有根本解决。

![三阶段协议](%E5%88%86%E5%B8%83%E5%BC%8F.assets/%E4%B8%89%E9%98%B6%E6%AE%B5%E5%8D%8F%E8%AE%AE.png)

引入了超时机制解决参与者阻塞的问题，超时后本地提交，2pc只有协调者有超时机制

-   第一阶段：CanCommit阶段，协调者询问事务参与者，是否有能力完成此次事务
    -   如果都返回yes，则进入第二阶段
    -   有一个返回no或等待响应超时，则中断事务，并向所有参与者发送abort请求
-   第二阶段：PreCommit阶段，此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事务操作。参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待协调者的下一步指令。
-   第三阶段：DoCommit阶段，在阶段二中如果所有参与者节点都返回了Ack，那么协调者就会从“预提交状态”转变为“提交状态”。然后向所有的参与者节点发送“doCommit”请求，参与者节点在收到提交请求后就会各自执行事务提交操作，并向协调者节点反馈“Ack”消息，协调者收到所有参与者的Ack消息后完成事务。相反，如果有一个参与者节点未完成PreCommit的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送abort请求，从而中断事务。



TCC（补偿事务）：Try，Confirm，Cancel

针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作

Try操作做业务检查及资源预留，Confirm做业务确认操作，Cancel实现一个与Try相反的操作即回滚操作。TM首先发起所有的分支事务的try操作，任何一个分支事务的try操作执行失败，TM将会发起所有分支事务的Cancel操作，若try操作全部成功，TM将会发起所有分支事务的Confirm操作，其中Confirm/Cancel操作若执行失败，TM会进行重试。

TCC模型对业务的侵入性较强，改造的难度较大，每个操作都需要有`try`，`confirm`，`cancel`三个接口实现。

`confirm`和`cancel`接口还必须实现幂等性。



消息队列的事务消息：

-   发送prepare消息到消息中间件
-   发送成功后，执行本地事务
    -   如果事务执行成功，则commit，消息中间件将消息下发至消费端（commit前，消息不会被消费）
    -   如果事务执行失败，则回滚，消息中间件将这条prepare消息删除
-   消费端接收到消息进行消费，如果消费失败，则不断重试

### 37. 分布式事务解决方案

1.  基于XA协议：两阶段提交和三阶段提交，需要数据库层面支持
2.  基于事务补偿机制：TCC，基于业务层面实现
3.  本地消息表：基于本地数据库+mq，维护本地状态（进行中），通过mq调用服务，完成后响应一条消息回调，将状态改成完成。需要配合定时任务扫表，重新发送消息调用服务，需要保证幂等。
4.  基于事务消息：mq

### 38. 分布式锁的使用场景？有哪些实现方案？

在单体架构中，多个线程都是属于同一个进程的，所以在线程并发执行时，遇到资源竞争时，可以利用ReenTrantLock，Synchroized等技术来作为锁，来控制共享资源的使用。

而在分布式架构中，多个线程是可能处于不同进程中的，而这些线程并发执行遇到资源竞争时，利用ReenTrantLock，Synchroized等技术是没办法来控制多个进程中的线程的，所以需要分布式锁，意思就是，需要一个分布式锁生成器，分布式系统中的应用程序都可以来使用这个生成器所提供的锁，从而达到多个进程中的线程使用同一把锁。

目前主流的分布式锁的实现方案有两种：

1.  Zookeeper：利用的是Zookeeper的临时节点，顺序节点，watch机制来实现的，Zookeeper分布式锁的特点是高一致性，因为Zookeeper保证的是CP，所以由它实现的分布式锁更可靠，不会出现混乱。
2.  redis：利用redis的setnx，lua脚本，消费订阅等机制来实现的，redis分布式锁的特点是高可用，因为redis保证的是AP，所以由它实现的分布式锁可能不可靠，不稳定（一旦redis中的数据出现了不一致），可能出现多个客户端同时加到锁的情况。

### 39. 分布式缓存寻址算法

-   hash算法：根据key进行hash函数运算，结果对分片数取模，确定分片

    适合固定分片数的场景

    扩展分片或者减少分片时，所有数据都需要重新计算分片，存储

-   一致性hash：将整个hash值的区间组织成一个闭合的圆环，计算每台服务器的hash值，映射到圆环中。使用相同的hash算法计算数据的hash值，映射到圆环，顺时针寻找，找到的第一个服务器就是数据存储的服务器。

    新增及减少节点时只会影响节点到它逆时针最近的一个服务器之间的值

    存在hash环倾斜的问题，即使服务器分布不均匀，可以通过虚拟节点解决

-   hash slot：将数据与服务器隔离开，数据与slot映射，slot与服务器映射，数据进行hash决定存放的slot新增及删除节点时，将slot进行迁移即可。

### 40. 分布式架构下，Session共享方案

1.  采用无状态服务，抛弃session

2.  存入cookie（有安全风险）

3.  服务器之间进行session同步，这样可以保证每个服务器上都有全部的session信息，不过当服务器数量比较多的时候，同步是会有延迟甚至同步失败。

4.  IP绑定策略

    使用Nginx（或其他复杂均衡软硬件）中的IP绑定策略，同一个IP只能在指定的同一个机器访问，但是这样做是取了负载均衡的意义，当挂掉一台服务器的时候，会影响一批用户的使用，风险很大；

5.  使用Redis存储

    把Session放到Redis中存储，虽然架构上变得复杂，并且需要多访问一次Redis，但是这种方案带来的好处也是很大的；

    -   实现了Session共享；
    -   可以水平扩展（增加Redis服务器）；
    -   服务器重启Session不丢失（不过也要注意Session在Redis中的刷新/失效机制）；
    -   不仅可以跨服务器Session共享，甚至可以跨平台（例如网页端和APP端）

### 41. 分布式ID生成方案

-   UUID

    ```java
    当前日期和时间  时间戳
    时钟序列       计数器
    全局唯一的IEEE机器识别号，如果有网卡，从网卡MAC地址获得，没有网卡以其他方式获得。
    
    优点：代码简单，性能好（本地生成，没有网络消耗），保证唯一（相对而言，重复概念极低可以忽略）
    缺点：
        每次生成的ID都是无序的，而且不是全数字，且无法保证趋势递增。
        uuid生成的是字符串，字符串存储性能差，查询效率慢，写的时候由于不能产生顺序append操作，需要进行insert操作，导致频繁的页分裂，这种操作在记录占用空间比较大的情况下，性能下降比较大，还会增加读取磁盘次数
        uuid长度过长，不适用于存储，耗费数据库性能
        ID无一定业务含义，可读性差
        有信息安全问题，有可能泄漏mac地址
    ```

    

-   数据库自增序列

    ```java
    单机模式：
    优点：
    	实现简单，依靠数据库即可，成本小
    	ID数字化，单调自增，满足数据库存储和查询性能
    	具有一定的业务可读性，（结合业务code）
    缺点：
    	强依赖DB，存在单点问题，如果数据库宕机，则业务不可用
    	DB生成ID性能有限，单点数据库压力大，无法扛高并发场景
    	信息安全问题，比如暴露订单量，utl查询改一下id就可以查到别人的订单
    	
    数据库高可用：多住模式做负载，基于序列的起始值和步长设置，不同的初始值，相同的步长，步长大于节点数
    优点：
    	解决了ID生成的单点问题，同时平衡了负载
    缺点：
    	系统扩容困难：系统定义好步长之后，增加机器之后调整步长困难
    	数据库压力大：每次获取一个ID都必须写一次数据库
    	主从同步的时候：电商下单 -> 支付insert master db select 数据，因为数据同步延迟导致查不到这个数据，加cache（不是最好的解决方式）数据要求比较严谨的话查master主库
    ```

    

-   Leaf-segment

    ```
    采用每次获取一个ID区间段的方式来解决，区间段用完之后再去数据库获取新的号段，这样一来可以大大减轻数据库的压力
    核心字段：biz_tag，max_id，step
    biz_tag用来区分业务，max_id表示该biz_tag目前所被分配的ID号段的最大值，step表示每次分配的号段长度，原来每次获取ID都要访问数据库，现在只需要把step设置的足够合理如1000，那么现在可以在1000个ID用完之后再去访问数据库
    优点：
    	扩展灵活，性能强能够撑起大部分业务场景
    	ID号码是趋势递增的，满足数据库存储和查询性能要求
    	可用性高，即使ID生成服务器不可用，也能够使得业务在短时间内可用，为排查问题争取时间
    缺点：
    	可能存在多个节点同时请求ID区间的情况，依赖DB
    	
    双buffer：将获取一个号段的方式优化成获取两个号段，在一个号段用完之后不用立马去更新号段，还有一个缓存号段备用，这样能够有效解决这种冲突问题，而且采用双buffer的方式，在当前号段消耗了10%的时候就去检查下一个号段有没有准备好，如果没有准备好就去更新下一个号段，当当前号段用完了就切换到下一个已经缓存好的号段去使用，同时在下一个号段消耗到10%的时候，又去监测下一个号段有没有准备好，如此往复。
    优点：
    	基于JVM存储双buffer的号段，减少了数据库查询，减少了网络依赖，效率更高
    缺点：
    	segment号段长度是固定的，业务量大时可能会频繁更新号段，因为原本分配的号段会一下用完
    	如果号段长度设置的过长，但凡缓存中有号段没有消耗完，其他节点重新获取的号段与之前相比可能跨度会很大，动态调整step
    ```

    

-   基于Redis，MongoDB，Zookeeper等中间件生成

-   雪花算法

    ```java
    生成一个64bit的整型数字
    第一位符号固定为0，41位时间戳，10位workId，12位序列号
    位数可以有不同的实现
    优点：
    	每个毫秒值包含的ID值很多，不够可以变动位数来增加，性能佳（依赖workId的实现）。
    	时间戳值在高位，中间是固定的机器码，自增的序列在低位，整个ID是趋势递增的。
    	能够根据业务场景数据库节点布置灵活调整bit位划分，灵活度高。
    缺点：
    	强依赖与机器时钟，如果时钟回拨，会导致重复的ID生成，所以一般基于此的算法发现时钟回拨，都会抛异常处理，阻止ID生成，这可能导致服务不可用。
    ```

    

### 42. 对比两阶段，三阶段有哪些改进

### 43. 定时任务实现原理

优先队列：基于小丁对实现，每次新增任务需要进行堆化，取任务时取堆顶元素，调整堆架构，时间复杂度是O(logN)。

时间轮算法：是一个唤醒队列，按照时间的单位区分，每个时间单位里面是一个链表，用来存储定时任务，像时钟一样轮询环形队列，取出链表中的任务执行，如果超出了唤醒队列的时间粒度，可以使用多级时间轮，即使用不同维度的时间单位，就跟时钟或者水表一样，这一层的走了一圈，下一层的才走了一格，时间复杂度为O(1)

### 44. Dubbo如何做系统交互的

Dubbo底层是通过RPC来完成服务和服务之间的调用的，Dubbo支持很多协议，比如默认的dubbo协议，比如http协议，比如rest都是支持的，他们的底层使用的技术是不太一样的。比如dubbo协议底层使用的是netty，也可以使用mina，http协议底层使用的tomcat或jetty。

服务消费者在调用某个服务时，会将当前所调用的服务接口信息，当前方法信息，执行方法所传入的入参信息等组装为一个Invocation对象，然后不同的协议通过不同的数据组织方式和传输方式将这个对象传送给服务提供者，提供者接收到这个对象后，找到对应的服务实现，利用反射执行对应的方法，得到方法结果后再通过网络响应给服务消费者。

Dubbo在这个调用过程中还做了很多其他的设计，比如服务容错，负载均衡，Filter机制，动态路由机制等，让Dubbo能处理更多企业中的需求。

### 45. Dubbo的负载均衡策略

Dubbo目前支持：

1.  平衡加权轮询算法
2.  加权随机算法
3.  一致性哈希算法
4.  最小活跃数算法

### 46. Zookeeper领导者选举流程

1.  集群中各个节点首先都是观望状态，一开始都会投票给自己，认为自己比较适合作为Leader
2.  然后相互交互投票，每个节点会受到其他节点发过来的选票，然后pk，先比较zxid，zxid大者获胜，zxid如果相等则比较myid，myid大者获胜
3.  一个节点收到其他节点发过来的投票，经过pk后，如果pk输了，则改票，此节点就会投给zxid或者myid更大的节点，并将选票放入自己的投票箱中，并将新的选票发送给其他节点
4.  如果pk是平局则将接收到的选票放入自己的投票箱中
5.  如果pk赢了，则忽略所接收到的选票
6.  当然一个节点将一张选票放入到自己的投票箱之后，就会从投票箱中统计票数，看是否超过一半的节点都和自己所投的节点是一样的，如果超过半数，那么则认为当前自己所投的节点是Leader
7.  集群中每个节点都会经过同样的流程，pk的规则也是一样的，一旦改票就会告诉给其他服务器，所以最终各个节点中的投票箱也将是一样的，所以各个节点最终选出来的leader也是一样的，这样集群的leader就选举出来了。

### 47. 分布式锁实现

分布式锁要解决的问题的本质是：能够对分布在多台机器中的线程对共享资源的互斥访问，在这个原理上可以有很多的实现方式：

1.  基于Mysql，分布式环境中的线程连接同一个数据库，利用数据库中的行锁来达到互斥访问，但是Mysql的加锁和释放锁的性能会比较低，不适合真正的实际生产环境
2.  基于Zookeeper，Zookeeper中的数据是存在内存中的，所以相对于Mysql性能上是适合实际环境的，并且基于Zookeeper的顺序节点，临时节点，Watch机制能非常好的来实现分布式锁
3.  基于Redis，Redis中的数据也是在内存中的，基于Redis的消费订阅功能，数据超时时间，lua脚本等功能，也能很好的实现分布式锁

### 48. Zookeeper集群中的节点之间的数据是如何同步的

1.  首先集群启动时，会先进行leader选举，确定哪个节点是leader，哪些节点是follower和observer
2.  然后leader会和其他节点进行数据同步，采用发送快照和发送diff日志的方式
3.  集群在工作过程中，所有的写请求都会交给leader节点来进行处理，从节点只能处理读请求
4.  Leader节点收到一个写请求时，会通过两阶段机制来处理
5.  Leader节点会将该写请求对应的日志发送给其他Follower节点，并等待Follower节点持久化日志成功
6.  Fllower节点收到日志后会进行持久化，如果持久化成功则发送一个Ack给Leader节点
7.  当Leader节点收到半数以上的Ack后，就会开始提交，先更新Leader节点本地的内存数据
8.  然后发送commit命令给Follower节点，Follower节点收到commit命令后就会更新各自本地内存数据
9.  同时Leader节点还是讲当前写请求直接发送给Observer节点，Observer节点收到Leader发过来的写请求后直接执行更新本地内存数据
10.  最后Leader节点返回客户端写请求响应成功
11.  通过同步机制和两阶段提交机制来达到集群中节点数据一致

### 49. zk中一个客户端修改了某个节点的数据，其他客户端能够马上获取到这个最新数据吗？

zk：CP（强一致性），目标是一个分布式的协调系统，用于进行资源的统一管理

当节点crash后，需要进行leader的选举，在这个期间内，zk服务是不可用的



eureka：AP（高可用），目标是一个服务注册发现系统，专门用于微服务的服务发现注册

Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册时如果发现连接失败，会自动切换至其他节点，只要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）

同时当Eureka的服务端发现85%以上的服务都没有心跳的话，它就会认为自己的网络出了问题，就不会从服务列表中删除这些失去心跳的服务，同时Eureka的客户端也会缓存服务信息，Eureka对于服务注册发现来说是非常好的选择。

### 50. zk实际如何存储dubbo生产者和消费者信息

### 51. zk分布式锁实现原理

-   上来直接创建一个锁节点下的一个接一个的临时顺序节点
-   如果自己不是第一个节点，就对自己上一个几点加监听器
-   只要上一个节点释放锁，自己就排到前面去了，相当于是一个排队截止

而且用临时顺序节点，如果某个客户端创建临时顺序节点之后，自己宕机了，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队，解决了惊群效应。

### 52. zk的会话管理机制

客户端连接zk，有zk分配一个全局唯一的sessionId，客户端需要配置超时时间timeOut并传到zk，zk会据此计算会话下一次超时的时间点，zk根据这个时间点按照分桶策略进行分开存放，zk会给session设置一个isClosing属性，如果检测到超时会将该属性标记为关闭

会话状态：CONNECTING，CONNECTED，RECONNECTING，RECONNECTED，CLOSE

SessionTracker：zk中的会话管理器，负责会话的创建，管理和清理

-   sessionWithTimeout：一个ConcurrentHashMap，用来管理会话的超时时间
-   sessionsById：HashMap，维护sessionId到session的映射
-   sessionSets：HashMap，会话超时后进行归档，便于恢复和管理

ExpiractionTime = CurrentTime + SessionTimeout

SessionTracker根据ExpiractionTime将session进行分桶管理，同时按照一定的时间间隔进行定期检查。

客户端读写请求都可以将session的超时时间重置，SessionTracker会将session进行分桶迁移，如果没有读写请求，客户端需要发送ping心跳链接，否则session超时会被清除。

会话清理：

-   标记isClosing为关闭，此时该会话有新的请求也无法处理
-   发起会话关闭请求，同步到整个集群，使用提交的方式
-   手机需要清理的临时节点，先获取内存数据库中会话对应的临时节点集合，如果此时有删除节点的请求到达，将请求对应的节点路径从集合中移除，避免重复删除，如果有创建节点请求到达，则将请求中的路径添加到集合中
-   添加删除事务变更，将节点删除事务添加到outstandingChanges中，触发Watch
-   删除临时节点
-   移除会话
-   关闭连接

连接断开后客户端可以重连zk，如果该session未过期，session重新变为CONNECTED

如果有时间超过`sessionTimeout`，服务器则会进行会话的清理工作，如果此时zk客户端才恢复连接，则会受到State为Expired的`WatchedEvent`，并断开与服务器的连接

重连：断开后更换服务器链接，RECONNECTING状态，会将会话迁移到新连接的服务器上

当一个客户端发一个心跳请求各服务端，但是网络延时，导致服务端没有收到，过一会后，客户端连接上了另一个新的服务端，在这之后，之前的心跳被旧的服务端收到了，这时候旧的服务端会被提醒，当前session已经被转移了，然后旧的服务端会关闭这个连接。客户端一般不会感知到这个异常，因为就连接一般都会被关闭。但是还有一个特殊情况，两个客户端同时使用保存着的session id+密码来重新连接服务端，第一个连接成功，紧接着第二个又连接成功，这回导致第一个连接被关闭，然后就是这两个客户端无限重连了。

### 53. zk的数据模型和节点类型

数据模型：属性结构

zk维护的数据主要有：客户端的会话（session）状态及数据节点（dataNode）信息

zk在内存中构造了个DataTree的数据结构，维护着path到dataNode的映射以及dataNode间的树状层级关系

### 54. zk的数据同步原理

根据三个参数的大小对比结果，选择对应的数据同步方式。

-   peerLastZxid：Learner服务器（Follower或Observer）最后处理的zxid
-   minCommittedLog：Leader服务器proposal缓存队列committedLog中的最小的zxid
-   maxCommittedLog：Leader服务器proposal缓存队列committedLog中的最大的zxid

Zookeeper中数据同步一共有四类，如下：

-   DIFF：直接差异化同步

    peerLastZxid界于minCommitted和maxCommittedLog之间

-   TRUNC+DIFF：先回滚再差异化同步

    当Leader服务器发现某个Learner包含了一条自己没有的事务记录，那么就需要让该Learner进行事务回滚到Leader服务器上存在的。

-   TRUNC：仅回滚同步

    peerLastZxid大于maxCommitted，Leader会要求Learner回滚到ZXID值为maxCommitted对应的事务操作

-   SNAP：全量同步

    peerLastZxid小于minCommitted

在初始化阶段，Leader服务器会有限初始化以全量同步方式来同步数据

Learner先向Leader注册，上报peerLastZxid

### 55. zk的watch机制实现原理

```java
new Zookeeper(String connectString, int sessionTimeout, Watcher watcher)
// 这个watcher将作为整个Zookeeper会话期间的上下文，一直被保存在客户端ZKWatchManager的defaultWatcher
```

也可以动态添加watcher：getData()，exists，getChildren()；

客户端实现过程：

-   标记该会话是一个带有Watch事件的请求
-   通过DATAWatchRegistration类来保存watcher事件和节点的对应关系
-   客户端向服务器发送请求，将请求封装成一个Packet对象，并添加到一个等待发送队列outgoingQueue中

### 56. zk的初始化选举和崩溃选举过程

zxid：事务id，sid：节点id

先对比zxid，在对比sid，先投自己，选票内容（zxid，sid），遇强改投

投票箱：每个节点在本地维护自己和其他节点的投票信息，改投时需要更新信息，并广播

节点状态：

-   LOOKING，竞选状态
-   FOLLOWING，随从状态，同步Leader状态，参与投票
-   OBSERVER，观察状态，同步Leader状态，不参与投票
-   LEADING，领导者状态

初始化：没有历史数据，5个节点为例

-   节点1启动，此时只有一台服务器启动，它发出的请求没有任何响应，所以它的选举状态一直是LOOKING状态
-   节点2启动，它与节点1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以serverId值较大的服务器2胜出，但是由于没有达到半数以上，所以服务器1，2还是继续保持LOOKING状态
-   节点3启动，与1，2节点通信交互数据，服务器3成为服务器1，2，3中的Leader，此时有三台服务器选举了3，所以3成为Leader
-   节点4启动，理论上服务器4应该是服务器1，2，3，4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能切换为Follower
-   节点5启动，同4一样

崩溃选举：

-   变更状态，Leader故障后，Follower进入Looking状态
-   各节点投票，先投自己（zxid，sid），再广播投票
-   接收到投票，对比zxid和sid，如果本节点小，则将票改为接收的投票信息，并记录投票信息，重新广播，否则本节点大，则可不做处理
-   统计本地投票信息，超过半数，则切换为Leading状态并广播

### 57. Session的分布式方案

### 58. Spring Cloud和Dubbo的区别

底层协议：SpringCloud基于http协议，Dubbo基于Tcp协议，决定了Dubbo的性能相对比较好

注册中心：SpringCloud使用的Eureka，Dubbo推荐使用Zookeeper

模型定义：Dubbo将一个接口定义为一个服务，SpringCloud则是将一个应用定义为一个服务

SpringCloud是一个生态，Dubbo是SpringCloud生态中关于服务调用的一种解决方案（服务治理）

### 59. Quorum，WARO机制

waro：一种简单的副本控制协议，写操作时，只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。优先保证读，任何节点读到的数据都是最新数据，牺牲了更新服务的可用性，只要有一个副本宕机了，写服务就不会成功。但只要有一个节点存活，仍能提供读服务。

Quorum：10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据，可以保证读到了最新的数据。无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最新一次成功提交的副本数据。需要配合一个获取最新成功提交的版本号的metadata服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。

### 60. Dubbo中zk集群挂掉，发布者和订阅者还能通信吗

可以

因为当启动Dubbo容器时，消费者会去Zookeeper拉取注册的生产者地址列表，并将其缓存在本地。每次发起调用，都会按照本地的地址列表，以负载均衡的策略去进行调用。但是Zookeeper挂掉则后续新的生产者无法被消费者发现。

-   注册中心对等集群，任意一台宕机后，会自动切换到另一台
-   注册中心全部宕掉，服务提供者和消费者仍可以通过本地缓存通讯
-   服务提供者无状态，任一台宕机后，不影响使用
-   服务提供者全部宕机，服务消费者会无法使用，并无限次重连等待服务者回复。

### 61. Dubbo支持的注册中心有哪些

Zookeeper（官方推荐）

-   优点：支持分布式
-   缺点：受限于Zookeeper的特性

Multicast：组播协议允许将一台主机发送的数据通过网络路由器和交换机复制到多个加入此组播的主机，是一种一对多的通讯方式。每一台服务提供方和服务消费方都可以看做是注册中心的一部分。

-   优点：去中心化，不需要单独安装软件
-   缺点：Provider和Consumer和Registry不能跨机房（路由）
-   不需要启动任何中心节点，只要广播地址一样，就可以互相发现，组播受网络结构限制，只适合小规模应用或开发阶段使用。

Redis：

-   优点：支持集群，性能高
-   缺点：要求服务器时间同步，否则可能出现集群失败问题

Simple

-   优点：标准RPC服务，没有兼容问题
-   缺点：不支持集群

### 62. Dubbo支持哪些负载均衡策略

1.  随机：从多个服务提供者随机选择一个来处理本次请求，调用量越大则分布越均匀，并支持按权重设置随机概率
2.  轮询：依次选择服务提供者来处理请求，并支持按权重进行轮询，底层采用的是平滑加权轮询算法
3.  最小活跃调用数：统计服务提供者当前正在处理的请求，下次请求过来则交给活跃最小的服务器来处理
4.  一致性哈希：相同参数的请求总是会发到同一个服务提供者

### 63. Dubbo支持的协议

DUbbo协议（官方推荐协议）

-   采用NIO复用单一长连接，并使用线程池并发处理请求，减少握手和加大并发效率，性能较好（推荐使用）
-   大文件上传时，可能出现问题（不使用Dubbo文件上传）

RMI（Remote Method Invocation）协议

-   JDK自带的能力，可与原生RMI互操作，基于TCP协议，短连接

Hessian协议

-   可与原生Hession互操作，基于HTTP协议，对于数据包比较大的情况比较友好
-   需Hessian.jar支持，Http短连接的开销大，它的参数和返回值都需要实现Serializable接口

Http协议

WebService：基于CXF的Frontend-simple和Transports-http实现；基于WebService的远程调用协议

-   序列化：SOAP文本序列化
-   适用场景：系统集成，跨语言调用

Thrift：Thrift是Facebook捐给Apache的一个RPC框架

-   语言中立
-   平台中立

### 64. Dubbo是什么？能做什么？

Dubbo是阿里开源的基于Java的高性能RPC分布式服务框架，现已成为Apache基金会孵化项目，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案

Dubbo就是个服务框架，如果没有分布式的需求，是不需要的，只有在分布式的时候， 才有Dubbo这样的分布式服务框架的需求，本质上是个远程服务调用的分布式框架

其核心部分包含：

1.  远程通讯：提供对多种基于长连接的NIO框架抽象封装，包括多种线程模型，序列化，以及“请求-响应”模式的信息交换方式，透明化的远程方法调用，就像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。
2.  集群容错：提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持，可以在内网替代F5等硬件负载均衡器，降低成本，减少单点。
3.  自动发现：基于注册中心目录服务，服务自动注册与发现，不在需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。

### 65. Dubbo的整体架构设计及分层

五个角色：

-   注册中心registry：服务注册与发现
-   服务提供者provider：暴露服务
-   服务消费者consumer：调用远程服务
-   监控中心monitor：统计服务的调用次数和调用时间
-   容器container：服务允许容器

调用流程：

1.  container容器负责启动，加载，运行provider
2.  provider在启动时，向registry中心注册自己提供的服务
3.  consumer在启动时，想registry中心订阅自己所需的服务
4.  registry返回服务提供者列表给consumer，如果有变更，registry将基于长连接推送变更数据给consumer
5.  consumer调用provider服务，基于负载均衡算法进行调用
6.  consumer调用provider的统计，基于短链接定时每分钟一次统计到monitor

分层：

接口服务层（service）：面向开发者，业务代码，接口，实现等

配置层（config）：对外配置接口，以ServiceConfig和ReferenceConfig为中心

服务代理层（Proxy）：对生产者和消费者，dubbo都会产生一个代理类封装调用细节，业务层对远程调用无感

服务注册层（registry）：封装服务地址的注册和发现，以服务URL为中心

路由层（cluster）：封装多个提供者的路由和负载均衡，并桥接注册中心

监控层（monitor）：RPC调用次数和调用时间监控

远程调用层（Protocal）：封装RPC调用

信息交换层（Exchange）：封装请求响应模式，同步转异步

网络传输层（transport）：抽象mina和netty为统一接口，统一网络传输接口

数据序列化层（serialize）：数据传输的序列化和反序列化

### 66. Dubbo是如何完成服务导出的

1.  首先Dubbo会将@DubboService或@Service注解进行解析得到服务参数，包括定义的服务名，服务接口，服务超时时间，服务协议等，得到一个ServiceBean
2.  然后调用ServiceBean的export方法进行服务导出
3.  然后将服务信息注册到注册中心，如果有多个协议，多个注册中心，那么就将服务按单个协议，单个注册中心进行注册
4.  将服务信息注册到注册中心后，还会绑定一些监听器，监听动态配置中心的变更
5.  还会将服务协议启动对应的web服务或网络框架，比如tomcat，netty等

### 67. Dubbo如何完成服务引入

1.  @Reference注解来引入一个服务，Dubbo会将注解和服务的信息解析出来，得到当前所引用的服务名，服务接口是什么
2.  然后从注册中心进行查询服务信息，得到服务的提供者信息，并存在消费端的服务目录中
3.  并绑定一些监听器用来监听动态配置中心的变更
4.  然后根据查询得到的服务提供者信息生成一个服务接口的代理对象，并放入Spring容器中作为Bean

### 68. CAP，BASE理论

Consistency（一致性）：

更新操作返回给客户端后，所有节点在同一时间的数据完全一致

对于客户端来说，一致性是指并发访问时更新过的数据如何获取的问题

从服务端来说，则是更新如何复制分布到整个系统，以保证数据最终一致

Availability（可用性）：

即服务一直可用，而且是正常响应时间，系统能够很好的为用户服务，不出现用户曹组失败或者访问超时等用户体验不好的情况

Partition Tolerance（分区容错性）：

即分布式系统在遇到某个节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言没有什么体验上的影响。

CP和AP：分区容错是必须保证的，当发生网络分区的时候，如果要继续服务，那么强一致性和可用性只能2选1



BASE是Basically Available（基本可用），Soft State（软状态）和Eventually consistent（最终一致性）

基本可用：

-   响应时间上的损失
-   系统功能上的损失

软状态：数据同步允许一定的延迟

最终一致性：系统中所有的数据副本，在经过一段时候的同步后，最终能够达到一个一致的状态，不要求实时

### 69. Dubbo和Spring Cloud的对比

1.  Dubbo由于是二进制的传输，占用宽带更少
2.  Spring Cloud是http协议传输，带宽较多，同时使用http协议一般会使用json报文，消耗会更大
3.  dubbo开发难度大，原因的dubbo的jar依赖问题有很多大型工程无法解决
4.  springcloud的接口协议约定比较自由松散，需要有强有力的行政措施来限制接口无序升级
5.  dubbo是springcloud的一个子集，解决的是分布式中的服务调用问题，而springcloud提供了全套的解决方案

### 70. Dubbo集群容错策略

Failover Cluster失败自动切换：dubbo的默认容错方案，当调用失败时自动切换到其他可用的节点，具体的重试次数和间隔时间可通过引用服务的时候配置，默认重试次数为1也就是只调用一次

Failback Cluster失败自动恢复：在调用失败，记录日志和调用信息，然后返回空结果给consumer，并且通过定时任务每隔5秒对失败的调用进行重试

Failfast Cluster快速失败：只会调用一次，失败后立刻抛出异常

Failsafe Cluster失败安全：调用出现异常，记录日志不抛出，返回空结果

Forking Cluster并行调用多个服务提供者：通过线程池创建多个线程，并发调用多个provider，结果保存到阻塞队列，只要有一个返回结果，就会立刻返回

Broadcast Cluster广播模式：逐个调用provider，如果其中一台报错，在循环调用结束后，抛出异常

### 71. Dubbo架构设计

1.  Proxy服务代理层，支持JDK动态代理，javassist等代理机制
2.  Registry注册中心层，支持Zookeeper，Redis等作为注册中心
3.  Protocal远程调用层，支持Dubbo，Http等调用协议
4.  Transport网络传输层，支持Netty，mina等网络传输框架
5.  Serialize数据序列化层，支持json，hessian等序列化机制















































